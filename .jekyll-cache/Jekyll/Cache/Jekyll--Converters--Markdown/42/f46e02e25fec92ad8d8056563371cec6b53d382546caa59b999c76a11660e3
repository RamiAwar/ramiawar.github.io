I"=<h2 id="introduction-to-autoencoders">Introduction to Autoencoders</h2>

<p>Many great writeups about autoencoders have already been made, so I’ll just reference one of my favorites instead of writing one on my own, since I have nothing to add. Forget about the fact that most of the blogs use Keras, its so simple that it seems like pseudocode, so it shouldn’t be hard to understand.</p>

<p><a href="https://blog.keras.io/building-autoencoders-in-keras.html"><strong>Autoencoders in Keras - Keras Blog</strong></a></p>

<h2 id="cart-pole">Cart-Pole</h2>

<p>For every environment, I’ll be quickly describing the observations (outputs), the actions (inputs), and reward, while linking the github repo wiki page for that environment for more details.</p>

<h1 id="cart-pole-wikiwiki-link">[<strong>Cart-Pole Wiki</strong>][wiki-link]</h1>

<h4 id="observation">Observation</h4>

<p>For cart-pole, we have an observation vector of length 4, containing:</p>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>Observation</th>
      <th>Min</th>
      <th>Max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Cart Position</td>
      <td>-2.4</td>
      <td>2.4</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Cart Velocity</td>
      <td>-Inf</td>
      <td>Inf</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Pole Angle</td>
      <td>~ -41.8°</td>
      <td>~ 41.8°</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Pole Velocity At Tip</td>
      <td>-Inf</td>
      <td>Inf</td>
    </tr>
  </tbody>
</table>

<p>This observation vector is outputted by the environment at every step/iteration.</p>

<p>To get this observation, we need to feed the environment an action to then perform a step on.
Generating a random action that complies with the type and shape of data needed as an input is as simple as calling the action_space.sample() method on the environment object. I’ll provide an <a href="#action-space-sample">example below</a>.</p>

<h4 id="action">Action</h4>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>Action</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Push cart to the left</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Push cart to the right</td>
    </tr>
  </tbody>
</table>

<h4 id="reward">Reward</h4>

<p>Every step taken generates a reward of one, since we managed to balance the rod for longer.</p>

<h4 id="termination-conditions">Termination Conditions</h4>

<ol>
  <li>Pole Angle is more than ±12°</li>
  <li>Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)</li>
  <li>Episode length is greater than 200</li>
</ol>

<h4 id="solved-requirements">Solved Requirements</h4>

<p>The episode is considered solved when the average reward is greater than or equal to 195 over 100 consecutive trials.</p>

<p>These are all taken from the page linked above, for those of you who didn’t visit it.
They simply describe the gym environment we chose to work on.</p>

<p id="action-space-sample">
	To get started, let's run the environment and render it after a random action. 
</p>

<p>Notice that this produces a warning saying that any steps after the simulation ends are meaningless, and for this we use the return values of <code class="highlighter-rouge">env.step(action)</code>.</p>

<h2><br /></h2>
<p><br /></p>

<p><code class="highlighter-rouge">env.step(action)</code> returns four values:</p>

<ol>
  <li>Observation: object described previously, basically is the output of the environment, or the states in a sense.</li>
  <li>Reward: amount of reward achieved by previous action.</li>
  <li>Done: boolean indicating if episode is terminated. This should control when we stop issuing actions since they’d be meaningless if issued after <code class="highlighter-rouge">done</code> was returned <code class="highlighter-rouge">True</code>.</li>
  <li>Info: diagnostic information useful for debugging, which should not be used for learning.</li>
</ol>

<p>These were taken from [<strong>openai gym docs</strong>][gym-docs].</p>

<p>We will use this <code class="highlighter-rouge">done</code> indicator in our first attempt at solving this environment.</p>

<h2 id="-1"><br /></h2>
<p><br /></p>

<h2 id="solution-using-randomness">Solution using Randomness</h2>

<p>As discussed above, to consider the episode solved, we need an average reward equal to or greater than 195. Note that if this only guarantees that no termination conditions be met within 200 frames only. The algorithm might fail one frame after that, but it would still be considered a successful solution according to the environment’s criteria. You can deduce that succeeding over 200 frames doesn’t guarantee that your model will keep the rod up in real life. For that, there are many control systems solutions, such as PIDs, root locus, state space, … None of these have anything to do with learning though, so they’re not intended solutions by OpenAI.</p>

<p>Our observation vector x has four values, we define a weight vector w:</p>

<p>$ \textbf{x} = [ a\quad b\quad c\quad d ] , \quad \textbf{w} = $
<script type="math/tex">\begin{bmatrix}\alpha\\\beta\\\omega\\\zeta \end{bmatrix}</script></p>

<p>What we will try to do is generate a prediction (0 or 1) moving us either left or right, depending on these parameters and the weight vector. To do this, we take the following function:</p>

<script type="math/tex; mode=display">% <![CDATA[
f(x, w) = 
     \begin{cases}
       0 &\quad\text{if } \textbf{x}\cdot \textbf{w} < 0\\
       1 &\quad\text{if } \textbf{x}\cdot \textbf{w} \geq 0\\
     \end{cases} %]]></script>

<p>Given enough trials and errors, there exists several weight vectors $ \textbf{w} $ which would result in the cart balancing the pole for at least 200 frames. We will find these weight vectors by iterating over randomly generated vectors and save one that satisfies our goal.</p>

:ET